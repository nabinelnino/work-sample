{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fe9fdb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b95a7e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_stocks_model():\n",
    "    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "    # Define the data and model paths\n",
    "    data_dir = os.path.join(__location__, 'model')\n",
    "    ml_dir = os.path.join(data_dir, 'saved_model')\n",
    "    model_path = os.path.join(ml_dir, 'xgb_model_stocks.joblib')\n",
    "\n",
    "    # Create the necessary directories if they don't exist\n",
    "    if not os.path.exists(ml_dir):\n",
    "        os.makedirs(ml_dir)\n",
    "\n",
    "    # Get the absolute path of the data directory using __location__\n",
    "    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "    data_dir = os.path.abspath(os.path.join(__location__, 'data'))\n",
    "    # Construct the file path to calculated.parquet\n",
    "    file_path = os.path.join(data_dir, 'processed_data', 'Task2', 'stocks_MA_RM', 'calculated.parquet')\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.drop_duplicates(subset=['vol_moving_avg', 'Adj_close_rolling_med', 'Volume'], keep='first')\n",
    "    # Calculate the number of rows for 50% sample\n",
    "    n = int(len(df) * 0.5)\n",
    "    df = df.sample(n=n, random_state=42)\n",
    "\n",
    "    features = ['vol_moving_avg', 'Adj_close_rolling_med']\n",
    "    target = 'Volume'\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df[:train_size]\n",
    "    test_df = df[train_size:]\n",
    "\n",
    "    # Create DMatrix objects for train and test data\n",
    "    dtrain = xgb.DMatrix(train_df[features], label=train_df[target])\n",
    "    dtest = xgb.DMatrix(test_df[features], label=test_df[target])\n",
    "\n",
    "    # define parameter grid with different num_round values\n",
    "    param_grid = {'n_estimators': [50, 100, 150, 200]}\n",
    "    xgb_model = xgb.XGBRegressor(learning_rate=0.1, max_depth=3, subsample=0.8)\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(dtrain.get_data(), dtrain.get_label())\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)\n",
    "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "    # Define the XGBoost hyperparameters\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    eval_list = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "    bst = xgb.train(params, dtrain, num_boost_round = best_n_estimators, evals = eval_list, early_stopping_rounds= 500, verbose_eval= 50)\n",
    "\n",
    "    # Save the model to disk\n",
    "    joblib.dump(bst, model_path)\n",
    "\n",
    "    # Use the trained model to make predictions on the test data\n",
    "    y_true = test_df[target].values\n",
    "    y_pred = bst.predict(dtest)\n",
    "\n",
    "    # Calculate the evaluation metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "\n",
    "    # Create the data/ML directory if it doesn't exist\n",
    "    if not os.path.exists(os.path.join(__location__, 'model/logs')):\n",
    "        os.makedirs(os.path.join(__location__, 'model/logs'))\n",
    "\n",
    "    # Create the training_logs.txt file inside the data/ML directory and write to it\n",
    "    with open(os.path.join(__location__, 'model/logs/training_logs.txt'), 'a') as f:\n",
    "        f.write(f'Training stats of stocks data\\n')\n",
    "        f.write(f'Training started at: {datetime.now()}\\n')\n",
    "        f.write(f'Training set size: {len(train_df)}\\n')\n",
    "        f.write(f'Test set size: {len(test_df)}\\n')\n",
    "        f.write(f'Number of rounds: {bst.best_iteration}\\n')\n",
    "        f.write(f'Minimum MAE: {bst.best_score}\\n')\n",
    "        f.write(f'Final MAE: {mae}\\n')\n",
    "        f.write(f'Final MSE: {mse}\\n')\n",
    "        f.write(f'Final R2: {r2}\\n')\n",
    "        f.write(f'Final RMSE: {rmse}\\n')\n",
    "        f.write('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88c393",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_etfs_model():\n",
    "    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "    # Define the data and model paths\n",
    "    data_dir = os.path.join(__location__, 'model')\n",
    "    ml_dir = os.path.join(data_dir, 'saved_model')\n",
    "    model_path = os.path.join(ml_dir, 'xgb_model_etfs.joblib')\n",
    "\n",
    "    # Create the necessary directories if they don't exist\n",
    "    if not os.path.exists(ml_dir):\n",
    "        os.makedirs(ml_dir)\n",
    "\n",
    "    # Get the absolute path of the data directory using __location__\n",
    "    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "    data_dir = os.path.abspath(os.path.join(__location__, 'data'))\n",
    "    # Construct the file path to calculated.parquet\n",
    "    file_path = os.path.join(data_dir, 'processed_data', 'Task2', 'etfs_MA_RM', 'calculated.parquet')\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "    df.dropna(inplace=True)\n",
    "    df = df.drop_duplicates(subset=['vol_moving_avg', 'Adj_close_rolling_med', 'Volume'], keep='first')\n",
    "    # Calculate the number of rows for 50% sample\n",
    "    n = int(len(df) * 0.5)\n",
    "    df = df.sample(n=n, random_state=42)\n",
    "\n",
    "    features = ['vol_moving_avg', 'Adj_close_rolling_med']\n",
    "    target = 'Volume'\n",
    "\n",
    "    # Split the data into train and test sets\n",
    "    train_size = int(0.8 * len(df))\n",
    "    train_df = df[:train_size]\n",
    "    test_df = df[train_size:]\n",
    "\n",
    "    # Create DMatrix objects for train and test data\n",
    "    dtrain = xgb.DMatrix(train_df[features], label=train_df[target])\n",
    "    dtest = xgb.DMatrix(test_df[features], label=test_df[target])\n",
    "\n",
    "    # define parameter grid with different num_round values\n",
    "    param_grid = {'n_estimators': [50, 100, 150, 200]}\n",
    "    xgb_model = xgb.XGBRegressor(learning_rate=0.1, max_depth=3, subsample=0.8)\n",
    "    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "    grid_search.fit(dtrain.get_data(), dtrain.get_label())\n",
    "    print(\"Best parameters: \", grid_search.best_params_)\n",
    "    print(\"Best score: \", grid_search.best_score_)\n",
    "    best_n_estimators = grid_search.best_params_['n_estimators']\n",
    "    # Define the XGBoost hyperparameters\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': 'mae',\n",
    "        'max_depth': 6,\n",
    "        'learning_rate': 0.1,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    # Train the XGBoost model\n",
    "    eval_list = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "    bst = xgb.train(params, dtrain, num_boost_round=best_n_estimators, evals=eval_list, early_stopping_rounds=500,\n",
    "                    verbose_eval=50)\n",
    "\n",
    "    # Save the model to disk\n",
    "    joblib.dump(bst, model_path)\n",
    "\n",
    "    # Use the trained model to make predictions on the test data\n",
    "    y_true = test_df[target].values\n",
    "    y_pred = bst.predict(dtest)\n",
    "\n",
    "    # Calculate the evaluation metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
    "\n",
    "    __location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))\n",
    "\n",
    "    # Create the data/ML directory if it doesn't exist\n",
    "    if not os.path.exists(os.path.join(__location__, 'model/logs')):\n",
    "        os.makedirs(os.path.join(__location__, 'model/logs'))\n",
    "\n",
    "    # Create the training_logs.txt file inside the data/ML directory and write to it\n",
    "    with open(os.path.join(__location__, 'model/logs/training_logs.txt'), 'a') as f:\n",
    "        f.write(f'Training stats of etfs data\\n')\n",
    "        f.write(f'Training started at: {datetime.now()}\\n')\n",
    "        f.write(f'Training set size: {len(train_df)}\\n')\n",
    "        f.write(f'Test set size: {len(test_df)}\\n')\n",
    "        f.write(f'Number of rounds: {bst.best_iteration}\\n')\n",
    "        f.write(f'Minimum MAE: {bst.best_score}\\n')\n",
    "        f.write(f'Final MAE: {mae}\\n')\n",
    "        f.write(f'Final MSE: {mse}\\n')\n",
    "        f.write(f'Final R2: {r2}\\n')\n",
    "        f.write(f'Final RMSE: {rmse}\\n')\n",
    "        f.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}